{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJSvJwS24wn0"
   },
   "source": [
    "# **Final Project**\n",
    "\n",
    "## **Problem stament :**     \n",
    "\n",
    "The widespread dissemination of fake news and propaganda presents serious societal risks, including the erosion of public trust, political polarization, manipulation of elections, and the spread of harmful misinformation during crises such as pandemics or conflicts. From an NLP perspective, detecting fake news is fraught with challenges. Linguistically, fake news often mimics the tone and structure of legitimate journalism, making it difficult to distinguish using surface-level features. The absence of reliable and up-to-date labeled datasets, especially across multiple languages and regions, hampers the effectiveness of supervised learning models. Additionally, the dynamic and adversarial nature of misinformation means that malicious actors constantly evolve their language and strategies to bypass detection systems. Cultural context, sarcasm, satire, and implicit bias further complicate automated analysis. Moreover, NLP models risk amplifying biases present in training data, leading to unfair classifications and potential censorship of legitimate content. These challenges underscore the need for cautious, context-aware approaches, as the failure to address them can inadvertently contribute to misinformation, rather than mitigate it.\n",
    "\n",
    "\n",
    "\n",
    "Use datasets in link : https://drive.google.com/drive/folders/1mrX3vPKhEzxG96OCPpCeh9F8m_QKCM4z?usp=sharing\n",
    "to complete requirement.\n",
    "\n",
    "## **About dataset:**\n",
    "\n",
    "* **True Articles**:\n",
    "\n",
    "  * **File**: `MisinfoSuperset_TRUE.csv`\n",
    "  * **Sources**:\n",
    "\n",
    "    * Reputable media outlets like **Reuters**, **The New York Times**, **The Washington Post**, etc.\n",
    "\n",
    "* **Fake/Misinformation/Propaganda Articles**:\n",
    "\n",
    "  * **File**: `MisinfoSuperset_FAKE.csv`\n",
    "  * **Sources**:\n",
    "\n",
    "    * **American right-wing extremist websites** (e.g., Redflag Newsdesk, Breitbart, Truth Broadcast Network)\n",
    "    * **Public dataset** from:\n",
    "\n",
    "      * Ahmed, H., Traore, I., & Saad, S. (2017): \"Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques\" *(Springer LNCS 10618)*\n",
    "\n",
    "\n",
    "\n",
    "## **Requirement**\n",
    "\n",
    "A team consisting of three members must complete a project that involves applying the methods learned from the beginning of the course up to the present. The team is expected to follow and document the entire machine learning workflow, which includes the following steps:\n",
    "\n",
    "1. **Data Preprocessing**: Clean and prepare the dataset,etc.\n",
    "\n",
    "2. **Exploratory Data Analysis (EDA)**: Explore and visualize the data.\n",
    "\n",
    "3. **Model Building**: Select and build one or more machine learning models suitable for the problem at hand.\n",
    "\n",
    "4. **Hyperparameter set up**: Set and adjust the model's hyperparameters using appropriate methods to improve performance.\n",
    "\n",
    "5. **Model Training**: Train the model(s) on the training dataset.\n",
    "\n",
    "6. **Performance Evaluation**: Evaluate the trained model(s) using appropriate metrics (e.g., accuracy, precision, recall, F1-score, confusion matrix, etc.) and validate their performance on unseen data.\n",
    "\n",
    "7. **Conclusion**: Summarize the results, discuss the model's strengths and weaknesses, and suggest possible improvements or future work.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZlAJs4n4yfT"
   },
   "source": [
    "# Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-17T12:48:15.361536Z",
     "iopub.status.busy": "2025-06-17T12:48:15.361048Z",
     "iopub.status.idle": "2025-06-17T12:48:19.666832Z",
     "shell.execute_reply": "2025-06-17T12:48:19.665950Z",
     "shell.execute_reply.started": "2025-06-17T12:48:15.361515Z"
    },
    "id": "Sag__UElw91_",
    "outputId": "d0889c16-7acc-42c1-9324-bc1b945c4284",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting textsearch>=0.0.21 (from contractions)\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
      "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-17T12:48:19.668674Z",
     "iopub.status.busy": "2025-06-17T12:48:19.668335Z",
     "iopub.status.idle": "2025-06-17T12:48:24.911820Z",
     "shell.execute_reply": "2025-06-17T12:48:24.911175Z",
     "shell.execute_reply.started": "2025-06-17T12:48:19.668651Z"
    },
    "id": "KA7vmonq423g",
    "outputId": "883fb31b-4191-4173-e30e-21853d72c5cc",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=a567161af394d34dbce76589b0a745cf27884ae51324fdda3d26a477f44a6ae9\n",
      "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-17T12:48:24.913629Z",
     "iopub.status.busy": "2025-06-17T12:48:24.913421Z",
     "iopub.status.idle": "2025-06-17T12:48:27.856522Z",
     "shell.execute_reply": "2025-06-17T12:48:27.855825Z",
     "shell.execute_reply.started": "2025-06-17T12:48:24.913610Z"
    },
    "id": "A_8TpEQRINOi",
    "outputId": "1979769d-be9b-4817-ab95-34e2fc1e1b52",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (2.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILypBVMaQLUq"
   },
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:49:42.502404Z",
     "iopub.status.busy": "2025-06-17T12:49:42.501671Z",
     "iopub.status.idle": "2025-06-17T12:49:42.508627Z",
     "shell.execute_reply": "2025-06-17T12:49:42.508022Z",
     "shell.execute_reply.started": "2025-06-17T12:49:42.502373Z"
    },
    "id": "qmsu51HB45Iz",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import html\n",
    "import os\n",
    "import quopri\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import emoji\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect, LangDetectException\n",
    "from tqdm import tqdm\n",
    "\n",
    "import contractions\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    XLNetForSequenceClassification,\n",
    "    get_scheduler,\n",
    ")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import get_scheduler\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "execution": {
     "iopub.execute_input": "2025-06-17T12:49:46.203454Z",
     "iopub.status.busy": "2025-06-17T12:49:46.203191Z",
     "iopub.status.idle": "2025-06-17T12:49:48.744080Z",
     "shell.execute_reply": "2025-06-17T12:49:48.743472Z",
     "shell.execute_reply.started": "2025-06-17T12:49:46.203435Z"
    },
    "id": "7PyECqZ-46fx",
    "outputId": "1247cdfe-f834-4738-eaa4-f52c5f001478",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The head of a conservative Republican faction ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Transgender people will be allowed for the fir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The special counsel investigation of links bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Trump campaign adviser George Papadopoulos tol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>President Donald Trump called on the U.S. Post...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34970</th>\n",
       "      <td>34970</td>\n",
       "      <td>Most conservatives who oppose marriage equalit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34971</th>\n",
       "      <td>34971</td>\n",
       "      <td>The freshman senator from Georgia quoted scrip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34972</th>\n",
       "      <td>34972</td>\n",
       "      <td>The State Department told the Republican Natio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34973</th>\n",
       "      <td>34973</td>\n",
       "      <td>ADDIS ABABA, Ethiopia —President Obama convene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34974</th>\n",
       "      <td>34974</td>\n",
       "      <td>Jeb Bush Is Suddenly Attacking Trump. Here's W...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34975 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                               text\n",
       "0               0  The head of a conservative Republican faction ...\n",
       "1               1  Transgender people will be allowed for the fir...\n",
       "2               2  The special counsel investigation of links bet...\n",
       "3               3  Trump campaign adviser George Papadopoulos tol...\n",
       "4               4  President Donald Trump called on the U.S. Post...\n",
       "...           ...                                                ...\n",
       "34970       34970  Most conservatives who oppose marriage equalit...\n",
       "34971       34971  The freshman senator from Georgia quoted scrip...\n",
       "34972       34972  The State Department told the Republican Natio...\n",
       "34973       34973  ADDIS ABABA, Ethiopia —President Obama convene...\n",
       "34974       34974  Jeb Bush Is Suddenly Attacking Trump. Here's W...\n",
       "\n",
       "[34975 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_df = pd.read_csv(\"/kaggle/input/misinfo/DataSet_Misinfo_TRUE.csv\")\n",
    "true_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:49:48.745344Z",
     "iopub.status.busy": "2025-06-17T12:49:48.745119Z",
     "iopub.status.idle": "2025-06-17T12:49:50.939363Z",
     "shell.execute_reply": "2025-06-17T12:49:50.938792Z",
     "shell.execute_reply.started": "2025-06-17T12:49:48.745329Z"
    },
    "id": "FdFf6oSg475X",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43637</th>\n",
       "      <td>44422</td>\n",
       "      <td>The USA wants to divide Syria.\\r\\n\\r\\nGreat Br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43638</th>\n",
       "      <td>44423</td>\n",
       "      <td>The Ukrainian coup d'etat cost the US nothing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43639</th>\n",
       "      <td>44424</td>\n",
       "      <td>The European Parliament falsifies history by d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43640</th>\n",
       "      <td>44425</td>\n",
       "      <td>The European Parliament falsifies history by d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43641</th>\n",
       "      <td>44426</td>\n",
       "      <td>A leading FSB officer, Segey Beseda, said duri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43642 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                               text\n",
       "0               0  Donald Trump just couldn t wish all Americans ...\n",
       "1               1  House Intelligence Committee Chairman Devin Nu...\n",
       "2               2  On Friday, it was revealed that former Milwauk...\n",
       "3               3  On Christmas day, Donald Trump announced that ...\n",
       "4               4  Pope Francis used his annual Christmas Day mes...\n",
       "...           ...                                                ...\n",
       "43637       44422  The USA wants to divide Syria.\\r\\n\\r\\nGreat Br...\n",
       "43638       44423  The Ukrainian coup d'etat cost the US nothing ...\n",
       "43639       44424  The European Parliament falsifies history by d...\n",
       "43640       44425  The European Parliament falsifies history by d...\n",
       "43641       44426  A leading FSB officer, Segey Beseda, said duri...\n",
       "\n",
       "[43642 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_df = pd.read_csv(\"/kaggle/input/misinfo/DataSet_Misinfo_FAKE.csv\")\n",
    "fake_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:49:50.940919Z",
     "iopub.status.busy": "2025-06-17T12:49:50.940338Z",
     "iopub.status.idle": "2025-06-17T12:49:50.950883Z",
     "shell.execute_reply": "2025-06-17T12:49:50.950203Z",
     "shell.execute_reply.started": "2025-06-17T12:49:50.940895Z"
    },
    "id": "XqMBo7ly4-bE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Delete order column\n",
    "true_df = true_df.drop('Unnamed: 0', axis=1)\n",
    "fake_df = fake_df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-17T12:48:54.099022Z",
     "iopub.status.idle": "2025-06-17T12:48:54.099280Z",
     "shell.execute_reply": "2025-06-17T12:48:54.099166Z",
     "shell.execute_reply.started": "2025-06-17T12:48:54.099154Z"
    },
    "id": "orKgADru4_8N",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fake_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9B71gKh45BQR"
   },
   "outputs": [],
   "source": [
    "true_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oyr1ehMs5Cj6"
   },
   "outputs": [],
   "source": [
    "fake_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UedEPJBX5FEw"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ke2XC07Z5GkZ"
   },
   "source": [
    "- Xử lý giá trị null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzZQEtPk5Lku"
   },
   "outputs": [],
   "source": [
    "true_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8BC8NCR5M3x"
   },
   "outputs": [],
   "source": [
    "fake_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:49:54.745729Z",
     "iopub.status.busy": "2025-06-17T12:49:54.745466Z",
     "iopub.status.idle": "2025-06-17T12:49:54.759000Z",
     "shell.execute_reply": "2025-06-17T12:49:54.758190Z",
     "shell.execute_reply.started": "2025-06-17T12:49:54.745709Z"
    },
    "id": "njIAkAPN5NTY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "true_df = true_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2cz30lw5K4x"
   },
   "source": [
    "- Xử lý giá trị duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_8t6Blo5QBD"
   },
   "outputs": [],
   "source": [
    "true_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSaZ_iud5R7u"
   },
   "outputs": [],
   "source": [
    "fake_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:49:57.264925Z",
     "iopub.status.busy": "2025-06-17T12:49:57.264300Z",
     "iopub.status.idle": "2025-06-17T12:49:57.394492Z",
     "shell.execute_reply": "2025-06-17T12:49:57.393569Z",
     "shell.execute_reply.started": "2025-06-17T12:49:57.264901Z"
    },
    "id": "JNqDdvw55TSG",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "true_df = true_df.drop_duplicates()\n",
    "fake_df = fake_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpvYPH0eJm89"
   },
   "source": [
    "- Thêm label và gộp 2 tập dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:49:59.295954Z",
     "iopub.status.busy": "2025-06-17T12:49:59.295283Z",
     "iopub.status.idle": "2025-06-17T12:49:59.307924Z",
     "shell.execute_reply": "2025-06-17T12:49:59.307355Z",
     "shell.execute_reply.started": "2025-06-17T12:49:59.295928Z"
    },
    "id": "C6tDHB6eJnYB",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The head of a conservative Republican faction ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transgender people will be allowed for the fir...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The special counsel investigation of links bet...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump campaign adviser George Papadopoulos tol...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>President Donald Trump called on the U.S. Post...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68599</th>\n",
       "      <td>Apparently, the new Kyiv government is in a hu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68600</th>\n",
       "      <td>The USA wants to divide Syria.\\r\\n\\r\\nGreat Br...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68601</th>\n",
       "      <td>The Ukrainian coup d'etat cost the US nothing ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68602</th>\n",
       "      <td>The European Parliament falsifies history by d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68603</th>\n",
       "      <td>A leading FSB officer, Segey Beseda, said duri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68604 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      The head of a conservative Republican faction ...      1\n",
       "1      Transgender people will be allowed for the fir...      1\n",
       "2      The special counsel investigation of links bet...      1\n",
       "3      Trump campaign adviser George Papadopoulos tol...      1\n",
       "4      President Donald Trump called on the U.S. Post...      1\n",
       "...                                                  ...    ...\n",
       "68599  Apparently, the new Kyiv government is in a hu...      0\n",
       "68600  The USA wants to divide Syria.\\r\\n\\r\\nGreat Br...      0\n",
       "68601  The Ukrainian coup d'etat cost the US nothing ...      0\n",
       "68602  The European Parliament falsifies history by d...      0\n",
       "68603  A leading FSB officer, Segey Beseda, said duri...      0\n",
       "\n",
       "[68604 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_df['label'] = 1\n",
    "fake_df['label'] = 0\n",
    "\n",
    "df = pd.concat([true_df, fake_df], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:50:01.892410Z",
     "iopub.status.busy": "2025-06-17T12:50:01.892148Z",
     "iopub.status.idle": "2025-06-17T12:50:01.912075Z",
     "shell.execute_reply": "2025-06-17T12:50:01.911516Z",
     "shell.execute_reply.started": "2025-06-17T12:50:01.892392Z"
    },
    "id": "928LxVh0Jo4h",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Former Russian economy minister Alexei Ulyukay...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Republicans were just given a leg up over Demo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This has to be one of the best remix videos ev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In line with the new Language Law, Russian is ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JERUSALEM  —   A day after approving the const...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68599</th>\n",
       "      <td>The Super Bowl had not yet begun and Trump fan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68600</th>\n",
       "      <td>U.S. House Republicans on Friday won passage o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68601</th>\n",
       "      <td>Share on Facebook Share on Twitter Known to th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68602</th>\n",
       "      <td>A New Jersey man who worked at the World Trade...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68603</th>\n",
       "      <td>Turkey and Iran have agreed to discuss within ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68604 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      Former Russian economy minister Alexei Ulyukay...      1\n",
       "1      Republicans were just given a leg up over Demo...      0\n",
       "2      This has to be one of the best remix videos ev...      0\n",
       "3      In line with the new Language Law, Russian is ...      0\n",
       "4      JERUSALEM  —   A day after approving the const...      1\n",
       "...                                                  ...    ...\n",
       "68599  The Super Bowl had not yet begun and Trump fan...      0\n",
       "68600  U.S. House Republicans on Friday won passage o...      1\n",
       "68601  Share on Facebook Share on Twitter Known to th...      0\n",
       "68602  A New Jersey man who worked at the World Trade...      1\n",
       "68603  Turkey and Iran have agreed to discuss within ...      1\n",
       "\n",
       "[68604 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True) # Shuffle dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rb4Hormg5aWf"
   },
   "source": [
    "* Kiểm tra imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-17T12:48:54.107613Z",
     "iopub.status.idle": "2025-06-17T12:48:54.107871Z",
     "shell.execute_reply": "2025-06-17T12:48:54.107772Z",
     "shell.execute_reply.started": "2025-06-17T12:48:54.107760Z"
    },
    "id": "tvMmWtg25b-P",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "403QUP4q5e6s"
   },
   "source": [
    "=> Dữ liệu không bị imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vYaFeDU5kO0"
   },
   "source": [
    "- Xử lý các văn bản không phải là tiếng Anh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:50:04.616533Z",
     "iopub.status.busy": "2025-06-17T12:50:04.615861Z",
     "iopub.status.idle": "2025-06-17T12:56:39.844239Z",
     "shell.execute_reply": "2025-06-17T12:56:39.843425Z",
     "shell.execute_reply.started": "2025-06-17T12:50:04.616509Z"
    },
    "id": "v7gyy9NYOr_d",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  label     lang\n",
      "127                                   Florida for Trump!      0  unknown\n",
      "253    0 комментариев 0 поделились Фото: AP \\nКоммент...      0       ru\n",
      "294    0 комментариев 7 поделились \\n\"Это полный бред...      0       ru\n",
      "297    +++ Beim Jupiter! Spuren römischer Zivilisatio...      0       de\n",
      "433    0 комментариев 0 поделились \\n23 октября в Ниж...      0       ru\n",
      "...                                                  ...    ...      ...\n",
      "68104  Mittwoch, 16. November 2016 Neue App ruft auto...      0       de\n",
      "68347  +++ Muhten ihm einiges zu: Bauer soll Streit u...      0       de\n",
      "68388  — The Sun (@TheSun) 23. November 2016 Laut Fer...      0       de\n",
      "68406                       President Obama is a Muslim.      0       ca\n",
      "68444  Страна: Китай Заявления КНДР о завершении свое...      0       ru\n",
      "\n",
      "[646 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def safe_detect(x):\n",
    "    if isinstance(x, str) and x.strip() and len(x.strip()) > 20:\n",
    "        try:\n",
    "            return detect(x)\n",
    "        except LangDetectException:\n",
    "            return 'unknown'\n",
    "    return 'unknown'\n",
    "\n",
    "df['lang'] = df['text'].apply(safe_detect)\n",
    "non_english = df[df['lang'] != 'en']\n",
    "print(non_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:56:39.845920Z",
     "iopub.status.busy": "2025-06-17T12:56:39.845450Z",
     "iopub.status.idle": "2025-06-17T12:56:39.853521Z",
     "shell.execute_reply": "2025-06-17T12:56:39.852792Z",
     "shell.execute_reply.started": "2025-06-17T12:56:39.845902Z"
    },
    "id": "BC_W0QPzOxVy",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=\"lang\", axis=1) # Xóa cột phụ sau khi xử lý"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-m74wad8O_sX"
   },
   "source": [
    "=> Không xóa các dòng văn bản không phải tiếng Anh vì label 0 - fake news chiếm đa số"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IR7Z3W7_5srF"
   },
   "source": [
    "- Xử lý các văn bản với số từ ít hơn 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:56:39.854468Z",
     "iopub.status.busy": "2025-06-17T12:56:39.854237Z",
     "iopub.status.idle": "2025-06-17T12:56:41.606767Z",
     "shell.execute_reply": "2025-06-17T12:56:41.606062Z",
     "shell.execute_reply.started": "2025-06-17T12:56:39.854447Z"
    },
    "id": "qEqtwUXC5uJ8",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label\n",
      "127                                 Florida for Trump!      0\n",
      "288    A MUST watch video!https://youtu.be/-5Z-jJ2Z4bU      0\n",
      "772                                               Cool      0\n",
      "965                    That would be unconstitutional.      0\n",
      "1115                   Around 120,000 displaced people      1\n",
      "...                                                ...    ...\n",
      "67547                           TRUMP VICTORY FOR SURE      0\n",
      "67689                                        Brilliant      0\n",
      "67766                                  Good guy.\\n👍👍👍👍      0\n",
      "67797      https://www.youtube.com/watch?v=gqxwF-TeYas      0\n",
      "67834                                        Horseshit      0\n",
      "\n",
      "[170 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Đếm số từ trong mỗi dòng\n",
    "short_texts = df[df['text'].apply(lambda x: len(str(x).split()) < 5)]\n",
    "\n",
    "# In ra các dòng này\n",
    "print(short_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:56:41.608443Z",
     "iopub.status.busy": "2025-06-17T12:56:41.608247Z",
     "iopub.status.idle": "2025-06-17T12:56:41.615446Z",
     "shell.execute_reply": "2025-06-17T12:56:41.614931Z",
     "shell.execute_reply.started": "2025-06-17T12:56:41.608428Z"
    },
    "id": "bfxfwHQP5v6A",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>Around 120,000 displaced people</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20229</th>\n",
       "      <td>Republican Congressman Will Hurd</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24713</th>\n",
       "      <td>Ted Cruz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26250</th>\n",
       "      <td>Four U.S. senators</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28034</th>\n",
       "      <td>“On 1/20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31892</th>\n",
       "      <td>No.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40323</th>\n",
       "      <td>(Reuters)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57596</th>\n",
       "      <td>Jan 29 (Reuters)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65117</th>\n",
       "      <td>advertisement</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   text  label\n",
       "1115    Around 120,000 displaced people      1\n",
       "20229  Republican Congressman Will Hurd      1\n",
       "24713                          Ted Cruz      1\n",
       "26250                Four U.S. senators      1\n",
       "28034                          “On 1/20      1\n",
       "31892                               No.      1\n",
       "40323                         (Reuters)      1\n",
       "57596                  Jan 29 (Reuters)      1\n",
       "65117                     advertisement      1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_texts[short_texts['label']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnSviFeV5y5u"
   },
   "source": [
    "=> Vì các dòng có text dưới 5 kí tự không mang nhiều ý nghĩa nên loại bỏ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:56:41.616893Z",
     "iopub.status.busy": "2025-06-17T12:56:41.616207Z",
     "iopub.status.idle": "2025-06-17T12:56:43.386130Z",
     "shell.execute_reply": "2025-06-17T12:56:43.385368Z",
     "shell.execute_reply.started": "2025-06-17T12:56:41.616873Z"
    },
    "id": "A5p6Ku4H5xvq",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loại bỏ các dòng có số từ < 5\n",
    "df = df[df['text'].apply(lambda x: len(str(x).split()) >= 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yBEgaak51Rj"
   },
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kd9m-Xae52wF"
   },
   "source": [
    "- Làm sạch văn bản (lower, bỏ dấu câu, stopwords, stemming...) + Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:56:43.387104Z",
     "iopub.status.busy": "2025-06-17T12:56:43.386881Z",
     "iopub.status.idle": "2025-06-17T12:56:43.392301Z",
     "shell.execute_reply": "2025-06-17T12:56:43.391742Z",
     "shell.execute_reply.started": "2025-06-17T12:56:43.387081Z"
    },
    "id": "BqU7Oo2I54mM",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first running\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:56:43.393159Z",
     "iopub.status.busy": "2025-06-17T12:56:43.392981Z",
     "iopub.status.idle": "2025-06-17T12:56:43.406584Z",
     "shell.execute_reply": "2025-06-17T12:56:43.405925Z",
     "shell.execute_reply.started": "2025-06-17T12:56:43.393145Z"
    },
    "id": "ykzDhSM057Jk",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:56:43.407691Z",
     "iopub.status.busy": "2025-06-17T12:56:43.407474Z",
     "iopub.status.idle": "2025-06-17T12:59:54.054781Z",
     "shell.execute_reply": "2025-06-17T12:59:54.054158Z",
     "shell.execute_reply.started": "2025-06-17T12:56:43.407673Z"
    },
    "id": "K0AWDT2g58rk",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        former russian economy minister alexei ulyukay...\n",
       "1        republicans were just given a leg up over demo...\n",
       "2        this has to be one of the best remix videos ev...\n",
       "3        in line with the new language law, russian is ...\n",
       "4        jerusalem — a day after approving the construc...\n",
       "                               ...                        \n",
       "68599    the super bowl had not yet begun and trump fan...\n",
       "68600    u.s. house republicans on friday won passage o...\n",
       "68601    share on facebook share on twitter known to th...\n",
       "68602    a new jersey man who worked at the world trade...\n",
       "68603    turkey and iran have agreed to discuss within ...\n",
       "Name: clean_text, Length: 68434, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(row):\n",
    "    row = str(row).lower()\n",
    "\n",
    "    # Remove email headers\n",
    "    row = re.sub(r'(?i)\\b(from|to|cc|bcc|subject|date|return-path|message-id|thread-topic|thread-index|content-type|mime-version|boundary|received|x-[\\w-]+):.*', ' ', row)\n",
    "\n",
    "    # Remove mailto links\n",
    "    row = re.sub(r'mailto:[^\\s]+', ' ', row)\n",
    "\n",
    "    # Decode quoted-printable\n",
    "    row = quopri.decodestring(row.encode('utf-8')).decode('utf-8', errors='ignore')\n",
    "\n",
    "    # Unescape HTML entities\n",
    "    row = html.unescape(row)\n",
    "\n",
    "    # Strip HTML tags\n",
    "    if '<' in row and '>' in row:\n",
    "        row = BeautifulSoup(row, \"lxml\").get_text()\n",
    "\n",
    "    # Normalize\n",
    "    row = re.sub(r'[\\t\\r\\n]', ' ', row)\n",
    "    row = re.sub(r'[_~+\\-]{2,}', ' ', row)\n",
    "    row = re.sub(r\"[<>()|&©ø%\\[\\]\\\\~*\\$€£¥]\", ' ', row)\n",
    "    row = re.sub(r\"\\\\x[0-9a-fA-F]{2}\", ' ', row)\n",
    "    row = re.sub(r'(https?://)([^/\\s]+)([^\\s]*)', r'\\2', row)\n",
    "    row = re.sub(r'[a-f0-9]{16,}', ' ', row)\n",
    "    row = re.sub(r'([.?!])[\\s]*\\1+', r'\\1', row)\n",
    "    row = re.sub(r'\\s+', ' ', row)\n",
    "\n",
    "    # Remove code-like keywords\n",
    "    row = re.sub(r'\\b(function|var|return|typeof|window|document|eval|\\.split)\\b', ' ', row)\n",
    "\n",
    "    # Remove programming symbols\n",
    "    row = re.sub(r'[{}=<>\\[\\]^~|`#@*]', ' ', row)\n",
    "\n",
    "    # Remove all emoji\n",
    "    row = emoji.replace_emoji(row, replace='')\n",
    "\n",
    "    # Cut code JS minify or base36 encode\n",
    "    code_gibberish = re.search(r'[a-z0-9]{20,}', row)\n",
    "    if code_gibberish and len(row) - code_gibberish.start() > 50:\n",
    "        row = row[:code_gibberish.start()]\n",
    "\n",
    "    # Cut off JS/CDATA tail\n",
    "    cutoff = re.search(\n",
    "        r'(//\\s*!?\\s*cdata|function\\s*\\(|var\\s+[a-zA-Z]|window\\s*\\.\\s*|document\\s*\\.\\s*|this\\s*\\.)',\n",
    "        row\n",
    "    )\n",
    "    if cutoff and len(row) - cutoff.start() > 10:\n",
    "        row = row[:cutoff.start()]\n",
    "\n",
    "    row = re.sub(r'!+\\s*cdata\\s*!+', ' ', row, flags=re.IGNORECASE)\n",
    "\n",
    "    return row.strip()\n",
    "\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "df['clean_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nW7Xan1n6VtR"
   },
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xGnctfh_NtL"
   },
   "source": [
    "## Label Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fL2B5McW6Xyy"
   },
   "outputs": [],
   "source": [
    "def plot_label_distribution(df, label_col):\n",
    "    ax = sns.countplot(x=label_col, data=df, hue=label_col, palette='pastel', dodge=False)\n",
    "\n",
    "    counts = df[label_col].value_counts().sort_index()\n",
    "    for x, y in enumerate(counts.values):\n",
    "        ax.text(x, y, f'{y}', ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "    plt.legend(title='Label', labels=df[label_col].unique(),)\n",
    "    plt.title('Label Distribution')\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "plot_label_distribution(df, 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXhTsZeJ_R8d"
   },
   "source": [
    "Sự chênh lệch giữa hai nhãn là rất nhỏ (chỉ 448 mẫu), cho thấy tập dữ liệu khá cân bằng giữa hai lớp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLOPAw1V_Sqc"
   },
   "source": [
    "## Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXjwkukP_T7B"
   },
   "outputs": [],
   "source": [
    "df['word_count'] = df['clean_text'].apply(lambda x: len(x.split()))\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df, x='word_count', hue='label', multiple='dodge', bins=20)\n",
    "plt.title('Distribution of Word Count in True vs Fake News')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEyoJfqw_WSS"
   },
   "source": [
    "Đa số các bài viết (khoảng 30000 bài) có số từ từ 0 đến 5000, cho cả True News và Fake News, với nhãn 0 (True News) có phần vượt trội hơn. Rất ít bài viết có số từ vượt quá 10000, cho thấy phân bố tập trung chủ yếu ở các bài viết ngắn đến trung bình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BwaYi_K_X-o"
   },
   "outputs": [],
   "source": [
    "df['char_count'] = df['clean_text'].apply(lambda x: len(x))\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df, x='char_count', hue='label', multiple='dodge', bins=20)\n",
    "plt.title('Distribution of Character Count in True vs Fake News')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwFLXrMf_Zp6"
   },
   "source": [
    "Phần lớn các bài viết (khoảng 30000 bài) có số ký tự từ 0 đến 20000, với nhãn 0 (True News) chiếm ưu thế. Số lượng giảm mạnh sau 20000 ký tự, thể hiện sự tập trung ở các bài viết có số ký tự thấp đến trung bình."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fj5Df8jG_cT9"
   },
   "source": [
    "## Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0iBB2-T_bYU"
   },
   "outputs": [],
   "source": [
    "true_words = ' '.join(df[df['label'] == 1]['clean_text']).split()\n",
    "true_words = set(true_words)\n",
    "\n",
    "fake_words = ' '.join(df[df['label'] == 0]['clean_text']).split()\n",
    "fake_words = set(fake_words)\n",
    "\n",
    "common_words = true_words.intersection(fake_words)\n",
    "\n",
    "unique_true_words = true_words - common_words\n",
    "unique_fake_words = fake_words - common_words\n",
    "\n",
    "print(f\"Number of common words between true and fake news: {len(common_words)}\")\n",
    "print(f\"Number of unique words in true news: {len(unique_true_words)}\")\n",
    "print(f\"Number of unique words in fake news: {len(unique_fake_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCRIrsUa_fHC"
   },
   "outputs": [],
   "source": [
    "true_word_freq = Counter(true_words)\n",
    "most_common_true = true_word_freq.most_common(20)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=[word[1] for word in most_common_true], y=[word[0] for word in most_common_true])\n",
    "plt.title('Top 20 Most Common Words in True News')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qF3RZjSp_gxg"
   },
   "outputs": [],
   "source": [
    "fake_word_freq = Counter(fake_words)\n",
    "most_common_fake = fake_word_freq.most_common(20)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=[word[1] for word in most_common_fake], y=[word[0] for word in most_common_fake])\n",
    "plt.title('Top 20 Most Common Words in Fake News')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAkLl7OI_ibc"
   },
   "source": [
    "Cả hai đồ thị \"Top 20 Most Common Words in True News\" và \"Top 20 Most Common Words in Fake News\" đều thể hiện tần suất xuất hiện của các từ phổ biến nhất trong từng loại tin tức. Từ `the` dẫn đầu với tần suất cao nhất trong cả hai trường hợp, tiếp theo là `to`, `of`, và `and`, cho thấy đây là các từ chức năng phổ biến. True News có tần suất tối đa khoảng 1 triệu, trong khi Fake News có tần suất cao hơn đáng kể, lên đến gần 8 triệu, phản ánh mật độ từ cao hơn trong Fake News."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXBuVyLV_kKP"
   },
   "outputs": [],
   "source": [
    "common_word_freq = Counter(common_words)\n",
    "most_common_shared = common_word_freq.most_common(20)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=[word[1] for word in most_common_shared], y=[word[0] for word in most_common_shared])\n",
    "plt.title('Top 20 Most Common Words Shared Between True and Fake News')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqVGKXVf_mUJ"
   },
   "outputs": [],
   "source": [
    "true_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(true_words))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(true_wordcloud, interpolation='bilinear')\n",
    "plt.title('Word Cloud for True News')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDWpfCgv_n9f"
   },
   "outputs": [],
   "source": [
    "fake_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(fake_words))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(fake_wordcloud, interpolation='bilinear')\n",
    "plt.title('Word Cloud for Fake News')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H15ZgvzC_qNI"
   },
   "source": [
    "Cả hai đều có sự xuất hiện của `trump` và `clinton` với kích thước lớn, nhưng Fake News có thêm các từ liên quan đến phương tiện truyền thông (như `twitter`, `youtube`, `video`) và từ cảm xúc (như `good`, `attack`), gợi ý sự khác biệt về phong cách và nội dung so với True News chỉ tập trung vào các thuật ngữ chính trị và hành chính."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7G57yoDC_s1V"
   },
   "source": [
    "## n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_w0VeIAS_uSC"
   },
   "outputs": [],
   "source": [
    "def get_top_n_grams(corpus, ngram_range=(2, 2), n=None):\n",
    "    vec = CountVectorizer(ngram_range=ngram_range, stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DcrPAUB0_vVU"
   },
   "outputs": [],
   "source": [
    "top_positive_unigrams = get_top_n_grams(df[df['label'] == 1]['clean_text'], ngram_range=(1, 1), n=20)\n",
    "top_negative_unigrams = get_top_n_grams(df[df['label'] == 0]['clean_text'], ngram_range=(1, 1), n=20)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=[word[1] for word in top_positive_unigrams], y=[word[0] for word in top_positive_unigrams])\n",
    "plt.title('Top 20 Unigrams in True News')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Unigrams')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=[word[1] for word in top_negative_unigrams], y=[word[0] for word in top_negative_unigrams])\n",
    "plt.title('Top 20 Unigrams in Fake News')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Unigrams')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzrBzmNh_yRN"
   },
   "source": [
    "Trong True News, `said` dẫn đầu với tần suất cao nhất (gần 160,000), theo sau là `trump`, `mr`, `president`, và `new`, cho thấy sự tập trung vào phát ngôn và các nhân vật chính trị. Trong Fake News, `trump` đứng đầu với tần suất vượt trội (gần 80,000), tiếp theo là `people`, `said`, `clinton` và `president`, phản ánh sự chú trọng vào các nhân vật chính trị và công chúng.\n",
    "\n",
    "True News có tần suất tổng thể cao hơn (lên đến 160,000), trong khi Fake News có phạm vi tần suất thấp hơn (tối đa 80,000), nhưng danh sách từ đa dạng hơn với các thuật ngữ như `election` và `world`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umXnbIqZ_xaj"
   },
   "outputs": [],
   "source": [
    "top_positive_bigrams = get_top_n_grams(df[df['label'] == 1]['clean_text'], ngram_range=(2, 2), n=20)\n",
    "top_negative_bigrams = get_top_n_grams(df[df['label'] == 0]['clean_text'], ngram_range=(2, 2), n=20)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=[word[1] for word in top_positive_bigrams], y=[word[0] for word in top_positive_bigrams])\n",
    "plt.title('Top 20 Bigrams in True News')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Bigrams')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=[word[1] for word in top_negative_bigrams], y=[word[0] for word in top_negative_bigrams])\n",
    "plt.title('Top 20 Bigrams in Fake News')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Bigrams')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwTCEjKL_1ym"
   },
   "source": [
    "Cả hai đều có sự xuất hiện mạnh của `trump`, `clinton`, và `united states`, nhưng True News tập trung hơn vào các thuật ngữ chính thức (như `prime minister`, `supreme court`) với tần suất giảm đều, trong khi Fake News có thêm các từ liên quan đến truyền thông (như `twitter com`, `pic twitter`) và hình ảnh (như `featured image`, `getty images`), cho thấy sự khác biệt về phong cách và nguồn thông tin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2M7q-XXi_4Ka"
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vRxlXoUx_6OH"
   },
   "outputs": [],
   "source": [
    "true_reviews = df[df['label'] == 1]['clean_text']\n",
    "tfidf_vectorizer_true = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tfidf_true = tfidf_vectorizer_true.fit_transform(true_reviews)\n",
    "true_top_words = pd.DataFrame(tfidf_true.toarray(), columns=tfidf_vectorizer_true.get_feature_names_out()).mean().sort_values(ascending=False)[:20]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=true_top_words.values, y=true_top_words.index)\n",
    "plt.title('Top 20 TF-IDF Words in True News')\n",
    "plt.xlabel('TF-IDF Score')\n",
    "plt.ylabel('Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJPaMzj5_7vh"
   },
   "outputs": [],
   "source": [
    "fake_reviews = df[df['label'] == 0]['clean_text']\n",
    "tfidf_vectorizer_fake = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tfidf_fake = tfidf_vectorizer_fake.fit_transform(fake_reviews)\n",
    "fake_top_words = pd.DataFrame(tfidf_fake.toarray(), columns=tfidf_vectorizer_fake.get_feature_names_out()).mean().sort_values(ascending=False)[:20]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=fake_top_words.values, y=fake_top_words.index)\n",
    "plt.title('Top 20 TF-IDF Words in Fake News')\n",
    "plt.xlabel('TF-IDF Score')\n",
    "plt.ylabel('Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiCvHjEM_-Vy"
   },
   "source": [
    "Cả hai đều có sự xuất hiện mạnh của `trump`, `clinton`, `president`, và `said`, nhưng True News nhấn mạnh các thuật ngữ hành chính (như `government`, `states`) với điểm TF-IDF giảm đều, trong khi Fake News nổi bật với các từ như `hillary`, `obama`, và `russia`, gợi ý sự tập trung vào các cá nhân và sự kiện cụ thể."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGIhetpC_-3O"
   },
   "source": [
    "## Textual Feature Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYT4cNW0_9l7"
   },
   "outputs": [],
   "source": [
    "def get_polarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "df['polarity'] = df['clean_text'].apply(get_polarity)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df, x='polarity', hue='label', multiple='stack', bins=50, kde=True)\n",
    "plt.title('Polarity Distribution by Label')\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EE9sDhFnAB4H"
   },
   "source": [
    "Phân phối của cả hai nhãn (0 và 1) tập trung chủ yếu quanh giá trị Polarity gần 0, rất ít bài viết với Polarity cực đoan (dưới -0.75 hoặc trên 0.75) cho thấy phần lớn các bài viết có độ tích cực hoặc tiêu cực trung bình. Nhãn 0 (Fake News) có số lượng bài viết cao hơn đáng kể so với nhãn 1 (True News)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9u6x4BsAEGc"
   },
   "outputs": [],
   "source": [
    "def get_subjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "df['subjectivity'] = df['clean_text'].apply(get_subjectivity)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df, x='subjectivity', hue='label', multiple='stack', bins=50, kde=True)\n",
    "plt.title('Subjectivity Distribution by Label')\n",
    "plt.xlabel('Subjectivity')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1jZ5dMoAG47"
   },
   "source": [
    "Phân phối của cả hai nhãn (0 và 1) tập trung chủ yếu ở giá trị Subjectivity từ 0 đến 0.6, rất ít bài viết lớn hơn 0.6. Nhãn 0 (Fake News) chiếm ưu thế tổng thể và có số lượng bài viết cao vượt trội hơn nhãn 1 ở giá trị 0.1 với khoảng 3.000 bài viết."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzshoVNnAIbU"
   },
   "outputs": [],
   "source": [
    "def get_flesch_kincaid(text):\n",
    "    return textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "df['readability_score'] = df['clean_text'].apply(get_flesch_kincaid)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df, x='readability_score', hue='label', multiple='stack', bins=50, kde=True)\n",
    "plt.title('Readability Score Distribution for True vs Fake News')\n",
    "plt.xlabel('Flesch-Kincaid Grade Level')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.legend(labels=['Fake', 'True'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RAvmdnLAJ4i"
   },
   "source": [
    "Phân phối của cả tin thật (nhãn 1) và tin giả (nhãn 0) tập trung chủ yếu ở mức Flesch-Kincaid Grade Level từ 0 đến 40, cả hai loại tin đều có số lượng giảm mạnh khi Flesch-Kincaid Grade Level tăng trên 40, với rất ít bài viết ở mức trên 80, cho thấy cả hai loại đều có mức độ dễ đọc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbhelCr9ANNs"
   },
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yq5lUCYvAOTx"
   },
   "outputs": [],
   "source": [
    "def count_punctuation(text, punct):\n",
    "    return text.count(punct)\n",
    "\n",
    "df['exclamation_count'] = df['clean_text'].apply(lambda x: count_punctuation(x, '!'))\n",
    "df['question_count'] = df['clean_text'].apply(lambda x: count_punctuation(x, '?'))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df, x='exclamation_count', hue='label', multiple='stack', bins=30)\n",
    "plt.title('Exclamation Mark (!) Distribution by Label')\n",
    "plt.xlabel('Number of Exclamation Marks')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df, x='question_count', hue='label', multiple='stack', bins=30)\n",
    "plt.title('Question Mark (?) Distribution by Label')\n",
    "plt.xlabel('Number of Question Marks')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ri4YRqhkARYd"
   },
   "outputs": [],
   "source": [
    "df = df[['text', 'label', 'clean_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5FEHQyZ6BbZ"
   },
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWZ2HCyWR6zd"
   },
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:59:54.055661Z",
     "iopub.status.busy": "2025-06-17T12:59:54.055482Z",
     "iopub.status.idle": "2025-06-17T12:59:54.059413Z",
     "shell.execute_reply": "2025-06-17T12:59:54.058712Z",
     "shell.execute_reply.started": "2025-06-17T12:59:54.055648Z"
    },
    "id": "1dMwo7LNR-jI",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmUqkSlh6DBz"
   },
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:59:54.061793Z",
     "iopub.status.busy": "2025-06-17T12:59:54.061602Z",
     "iopub.status.idle": "2025-06-17T12:59:54.081312Z",
     "shell.execute_reply": "2025-06-17T12:59:54.080782Z",
     "shell.execute_reply.started": "2025-06-17T12:59:54.061779Z"
    },
    "id": "qaMNYg9OSBex",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X = df['clean_text'].tolist()\n",
    "y = df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:59:54.082186Z",
     "iopub.status.busy": "2025-06-17T12:59:54.081958Z",
     "iopub.status.idle": "2025-06-17T12:59:54.159538Z",
     "shell.execute_reply": "2025-06-17T12:59:54.158834Z",
     "shell.execute_reply.started": "2025-06-17T12:59:54.082171Z"
    },
    "id": "utsC--UE6iWA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=df['label'])\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=RANDOM_STATE, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cg7wwEteKlXO"
   },
   "source": [
    "* Đếm số lượng nhãn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:59:54.160556Z",
     "iopub.status.busy": "2025-06-17T12:59:54.160322Z",
     "iopub.status.idle": "2025-06-17T12:59:54.168358Z",
     "shell.execute_reply": "2025-06-17T12:59:54.167770Z",
     "shell.execute_reply.started": "2025-06-17T12:59:54.160536Z"
    },
    "id": "YZbh0IWl6v7y",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label distribution: Counter({1: 24161, 0: 23742})\n",
      "Validation label distribution: Counter({1: 5178, 0: 5087})\n",
      "Test label distribution: Counter({1: 5178, 0: 5088})\n"
     ]
    }
   ],
   "source": [
    "print(\"Train label distribution:\", Counter(y_train))\n",
    "print(\"Validation label distribution:\", Counter(y_val))\n",
    "print(\"Test label distribution:\", Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZMw5cAh7AJ9"
   },
   "source": [
    "## The necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:59:54.169117Z",
     "iopub.status.busy": "2025-06-17T12:59:54.168915Z",
     "iopub.status.idle": "2025-06-17T12:59:54.181891Z",
     "shell.execute_reply": "2025-06-17T12:59:54.181307Z",
     "shell.execute_reply.started": "2025-06-17T12:59:54.169104Z"
    },
    "id": "bgdhow5h7q4U",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "  label_description = {\"0\": \"Fake News\", \"1\": \"True News\"}\n",
    "  print(\"Classification report: \\n\", classification_report(y_true , y_pred))\n",
    "\n",
    "  print(\"Confusion matrix: \\n\")\n",
    "  conf_matrix = confusion_matrix(y_true , y_pred)\n",
    "  plt.figure(figsize=(10, 7))\n",
    "  sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=list(label_description.values()), yticklabels=list(label_description.values()))\n",
    "  plt.xlabel('Predicted Class')\n",
    "  plt.ylabel('True Class')\n",
    "  plt.title('Confusion Matrix')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:59:54.182722Z",
     "iopub.status.busy": "2025-06-17T12:59:54.182509Z",
     "iopub.status.idle": "2025-06-17T12:59:54.196106Z",
     "shell.execute_reply": "2025-06-17T12:59:54.195457Z",
     "shell.execute_reply.started": "2025-06-17T12:59:54.182705Z"
    },
    "id": "8aXKjXOxT37m",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_curves(train_loss, val_loss, val_acc, title=\"Learning Curve\"):\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    # Loss\n",
    "    axes[0].plot(epochs, train_loss, label=\"Train loss\")\n",
    "    axes[0].plot(epochs, val_loss,   label=\"Val loss\")\n",
    "    axes[0].set_xlabel(\"Epoch\"); axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].set_title(\"Loss\"); axes[0].legend(); axes[0].grid(ls=\"--\", alpha=.4)\n",
    "\n",
    "    # Val accuracy\n",
    "    axes[1].plot(epochs, val_acc, label=\"Val acc\", color=\"tab:orange\")\n",
    "    axes[1].set_xlabel(\"Epoch\"); axes[1].set_ylabel(\"Accuracy\")\n",
    "    axes[1].set_title(\"Validation accuracy\")\n",
    "    axes[1].legend(); axes[1].grid(ls=\"--\", alpha=.4)\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:59:54.196872Z",
     "iopub.status.busy": "2025-06-17T12:59:54.196713Z",
     "iopub.status.idle": "2025-06-17T12:59:54.210411Z",
     "shell.execute_reply": "2025-06-17T12:59:54.209833Z",
     "shell.execute_reply.started": "2025-06-17T12:59:54.196860Z"
    },
    "id": "DL_sea3IK2Lp",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_score, pos_label=1, title=\"ROC Curve\"):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score, pos_label=pos_label)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "    ax.plot(fpr, tpr, color=\"tab:red\",\n",
    "            label=f\"User model (AUC = {roc_auc:.2f})\", lw=2)\n",
    "\n",
    "    # random\n",
    "    ax.plot([0, 1], [0, 1], \"k--\", lw=2)  # Đường chéo\n",
    "\n",
    "    # perfect\n",
    "    ax.plot([0, 0, 1], [0, 1, 1], color=\"green\",\n",
    "            label=\"Perfect model\", lw=1)\n",
    "\n",
    "    ax.set_xlim([0.0, 1.0]); ax.set_ylim([0.0, 1.02])\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.grid(alpha=0.3, ls=\"--\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqMoPAZm6zHn"
   },
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZW9yBpQ629U"
   },
   "source": [
    "### Model ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bji6tbYjRwH7"
   },
   "outputs": [],
   "source": [
    "stemmer    = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80nLEAP-R2Tl"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_filter(text):\n",
    "    text = contractions.fix(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return [stemmer.stem(w)\n",
    "            for w in tokens\n",
    "            if w.lower() not in stop_words\n",
    "            and w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWun664OC7Rz"
   },
   "outputs": [],
   "source": [
    "def build_ml_model(X, y):\n",
    "    model = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            tokenizer=tokenize_and_filter,\n",
    "            lowercase=False,\n",
    "            preprocessor=None,\n",
    "            token_pattern=None,\n",
    "            ngram_range=(1, 2)\n",
    "        )),\n",
    "        (\"svc\", SVC(kernel='linear'))\n",
    "    ])\n",
    "\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x53pgZBdwTlm"
   },
   "outputs": [],
   "source": [
    "model = build_ml_model(X_train, y_train)\n",
    "\n",
    "pred = model.predict(X_val)\n",
    "evaluate(y_val, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKdI9Y_YREpb"
   },
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    \"tfidf__ngram_range\": [(1, 1), (1, 2)],\n",
    "    \"tfidf__min_df\":      [2, 5, 10],\n",
    "    \"tfidf__max_df\":      [0.85, 0.9, 0.95],\n",
    "    \"tfidf__max_features\": [None, 50_000, 100_000],\n",
    "\n",
    "    \"svc__C\":            [0.1, 1, 2, 5],\n",
    "    \"svc__class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        tokenizer      = tokenize_and_filter,\n",
    "        lowercase      = False,   # ta đã xử lý trong tokenizer\n",
    "        preprocessor   = None,\n",
    "        token_pattern  = None     # tắt regex mặc định\n",
    "    )),\n",
    "    (\"svc\",  SVC(kernel=\"linear\", random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# Set up GridSearchCV\n",
    "gridsearch = GridSearchCV(pipeline, param_grid, cv=5, scoring=\"f1\", verbose=1)\n",
    "\n",
    "# Find the best hyperparameters\n",
    "gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found and the best cross-validation score\n",
    "print(\"Best Parameters:\", gridsearch.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", gridsearch.best_score_)\n",
    "\n",
    "# Save the fitted GridSearchCV object to a pkl file\n",
    "with open('best_svm.pkl', 'wb') as file:\n",
    "    pickle.dump(gridsearch, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1wsUy2RHU7va"
   },
   "outputs": [],
   "source": [
    "# Load the GridSearchCV object from the pickle file\n",
    "with open('best_svm.pkl', 'rb') as file:\n",
    "    loaded_gridsearch = pickle.load(file)\n",
    "\n",
    "print(\"Best Parameters:\", loaded_gridsearch.best_params_)\n",
    "\n",
    "best_model = loaded_gridsearch.best_estimator_\n",
    "\n",
    "y_score = best_model.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWnMVxSh74a5"
   },
   "source": [
    "### Model DL cơ bản"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtwE6NRH8mJT"
   },
   "source": [
    "#### 1. Multilevel-CNN\n",
    "- Bắt được đặc trưng từ từ, cụm từ, câu bằng các kernel kích thước khác nhau (3, 4, 5,...).\n",
    "- Phù hợp với dữ liệu dài + đa dạng, không phụ thuộc vào thứ tự quá dài như RNN.\n",
    "- Huấn luyện nhanh hơn LSTM, độ chính xác cao hơn CNN đơn thuần.\n",
    "\n",
    "#### 2. CNN + BiLSTM\n",
    "- CNN trích đặc trưng cục bộ, sau đó BiLSTM hiểu ngữ cảnh hai chiều (trước và sau).\n",
    "- Phù hợp cho dữ liệu có logic tuyến tính (như tin tức).\n",
    "- Độ chính xác cao, tuy chậm hơn Multilevel-CNN chút nhưng vẫn tốt nếu tối ưu đúng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWDeUpSR8oJu"
   },
   "source": [
    "#### Multilevel-CNN\n",
    "\n",
    "**Kiến trúc gợi ý:**\n",
    "\n",
    "Input (chuỗi văn bản)\n",
    "→ Embedding Layer\n",
    "→ Conv1D (kernel size 3) → GlobalMaxPool\n",
    "→ Conv1D (kernel size 4) → GlobalMaxPool\n",
    "→ Conv1D (kernel size 5) → GlobalMaxPool\n",
    "→ Concatenate\n",
    "→ Dense layers → Dropout (chưa có trong code)\n",
    "→ Output (Sigmoid / Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjg0PYCK8lid"
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "# Fit tokenizer trên dữ liệu train\n",
    "tokenizer = Tokenizer(num_words=5000) # giữ lại 5000 từ phổ biến nhất\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "# Convert văn bản thành câu và padding\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=512, padding='post', truncating='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=512, padding='post', truncating='post')\n",
    "\n",
    "# Convert to tensor\n",
    "X_train_tensor = torch.tensor(X_train_pad, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_pad, dtype=torch.long)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Dataset & DataLoader\n",
    "class TextDataset(Dataset):\n",
    "    '''\n",
    "    Tạo custom Dataset từ dữ liệu đã padding và label.\n",
    "    '''\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = TextDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = TextDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Định nghĩa model\n",
    "class MultilevelCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes=1):\n",
    "        super(MultilevelCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim) # Embedding layer\n",
    "        self.conv3 = nn.Conv1d(in_channels=embed_dim, out_channels=32, kernel_size=3)\n",
    "        self.conv4 = nn.Conv1d(in_channels=embed_dim, out_channels=32, kernel_size=4)\n",
    "        self.conv5 = nn.Conv1d(in_channels=embed_dim, out_channels=32, kernel_size=5)\n",
    "        self.fc = nn.Linear(32*3, 10)\n",
    "        self.out = nn.Linear(10, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "        x = x.permute(0, 2, 1) # (batch_size, embed_dim, seq_len)\n",
    "\n",
    "        x1 = F.relu(self.conv3(x)) # Conv1d với kernel_size = 3\n",
    "        x2 = F.relu(self.conv4(x)) # Conv1d với kernel_size = 4\n",
    "        x3 = F.relu(self.conv5(x)) # Conv1d với kernel_size = 5\n",
    "\n",
    "        x1 = F.max_pool1d(x1, x1.size(2)).squeeze(2)\n",
    "        x2 = F.max_pool1d(x2, x2.size(2)).squeeze(2)\n",
    "        x3 = F.max_pool1d(x3, x3.size(2)).squeeze(2)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3), 1) # Nối lại các features\n",
    "        x = F.relu(self.fc(x))\n",
    "        x = torch.sigmoid(self.out(x)) # Binary classification\n",
    "\n",
    "        return x\n",
    "\n",
    "# Model, loss, optimizer\n",
    "model = MultilevelCNN(vocab_size=5000, embed_dim=16).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 2\n",
    "wait = 0\n",
    "num_epochs = 20\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\", leave=True)\n",
    "    for batch_X, batch_y in loop:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X).squeeze(1)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # === Validation sau mỗi epoch ===\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    val_loss_total = 0\n",
    "    total = 0\n",
    "    val_loop = tqdm(test_loader, desc=f\"Epoch {epoch+1} [Val]\", leave=True)\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loop:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X).squeeze(1)\n",
    "            val_loss = criterion(outputs, batch_y)\n",
    "            val_loss_total += val_loss.item()\n",
    "\n",
    "            preds = (outputs > 0.5).float()\n",
    "            correct += (preds == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "            val_loop.set_postfix(val_loss=val_loss.item())\n",
    "\n",
    "    val_acc = correct / total\n",
    "    val_loss_avg = val_loss_total / len(test_loader)\n",
    "    total_loss_avg = total_loss / len(test_loader)\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {total_loss_avg:.4f} | Val Loss: {val_loss_avg:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "    # === Early stopping check ===\n",
    "    if val_loss_avg < best_val_loss:\n",
    "        best_val_loss = val_loss_avg\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJ86o9aT82Cz"
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "def predict_text(texts):\n",
    "    model.eval()\n",
    "    seq = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(seq, maxlen=512, padding='post', truncating='post')\n",
    "    tensor = torch.tensor(padded, dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tensor)\n",
    "        probs = outputs.cpu().numpy()\n",
    "        return probs, (probs > 0.5).astype(int)\n",
    "\n",
    "probs, preds = predict_text(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUnO7diO76eU"
   },
   "source": [
    "### Model DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:59:54.211366Z",
     "iopub.status.busy": "2025-06-17T12:59:54.211146Z",
     "iopub.status.idle": "2025-06-17T12:59:54.228355Z",
     "shell.execute_reply": "2025-06-17T12:59:54.227704Z",
     "shell.execute_reply.started": "2025-06-17T12:59:54.211352Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 2e-5\n",
    "epochs = 5\n",
    "patience = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:59:54.229155Z",
     "iopub.status.busy": "2025-06-17T12:59:54.228941Z",
     "iopub.status.idle": "2025-06-17T12:59:54.242534Z",
     "shell.execute_reply": "2025-06-17T12:59:54.242021Z",
     "shell.execute_reply.started": "2025-06-17T12:59:54.229141Z"
    },
    "id": "vnbDwTZw62TJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_tokenizer_and_model(model_name: str, num_labels: int):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:59:54.243525Z",
     "iopub.status.busy": "2025-06-17T12:59:54.243218Z",
     "iopub.status.idle": "2025-06-17T12:59:54.258012Z",
     "shell.execute_reply": "2025-06-17T12:59:54.257433Z",
     "shell.execute_reply.started": "2025-06-17T12:59:54.243505Z"
    },
    "id": "Q-IbcO-P6B_e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:59:54.258887Z",
     "iopub.status.busy": "2025-06-17T12:59:54.258706Z",
     "iopub.status.idle": "2025-06-17T12:59:54.272142Z",
     "shell.execute_reply": "2025-06-17T12:59:54.271591Z",
     "shell.execute_reply.started": "2025-06-17T12:59:54.258873Z"
    },
    "id": "KFc31g3v8DZn",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DataLoaderBuilder:\n",
    "    def __init__(self, dataset, batch_size=32, shuffle=True, num_workers=2):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def get_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "            num_workers=self.num_workers\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:59:54.273072Z",
     "iopub.status.busy": "2025-06-17T12:59:54.272846Z",
     "iopub.status.idle": "2025-06-17T12:59:54.294798Z",
     "shell.execute_reply": "2025-06-17T12:59:54.294184Z",
     "shell.execute_reply.started": "2025-06-17T12:59:54.273050Z"
    },
    "id": "keYc2bbk8CfC",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, model_name, lr=2e-5, epochs=5, patience=2, device='cuda'):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.model_name = model_name\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.patience = patience\n",
    "        self.device = device\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=lr)\n",
    "        self.scheduler = get_scheduler(\"linear\", self.optimizer, num_warmup_steps=0,\n",
    "                                       num_training_steps=len(train_loader) * epochs)\n",
    "        \n",
    "        # Track history\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(self.train_loader, desc=\"Training\"):\n",
    "            batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "            outputs = self.model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(self.train_loader)\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_loader, desc=\"Validating\"):\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                outputs = self.model(**batch)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                total_loss += loss.item()\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(batch['labels'].cpu().numpy())\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        return acc, avg_loss, all_preds, all_labels\n",
    "\n",
    "    def train(self):\n",
    "        best_loss = float('inf')\n",
    "        stop_count = 0\n",
    "        save_path = os.path.join(\"/kaggle/working\", f\"{self.model_name}_best.pt\")\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            print(f\"\\nEpoch {epoch}/{self.epochs}\")\n",
    "            train_loss = self.train_one_epoch()\n",
    "            val_acc, val_loss, _, _ = self.evaluate()\n",
    "\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.val_accuracies.append(val_acc)\n",
    "\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss:   {val_loss:.4f} | Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                stop_count = 0\n",
    "                torch.save(self.model.state_dict(), save_path)\n",
    "            else:\n",
    "                stop_count += 1\n",
    "                if stop_count >= self.patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "        # Reload best model\n",
    "        self.model.load_state_dict(torch.load(save_path))\n",
    "        \n",
    "        # Lưu lại lịch sử train/val\n",
    "        log_df = pd.DataFrame({\n",
    "            \"epoch\": list(range(1, len(self.train_losses) + 1)),\n",
    "            \"train_loss\": self.train_losses,\n",
    "            \"val_loss\": self.val_losses,\n",
    "            \"val_acc\": self.val_accuracies\n",
    "        })\n",
    "        log_df.to_csv(f\"/kaggle/working/{self.model_name}_training_log.csv\", index=False)\n",
    "        print(\"Training log saved.\")\n",
    "\n",
    "        return self.train_losses, self.val_losses, self.val_accuracies\n",
    "\n",
    "    def test_model(self, test_loader):\n",
    "        \"\"\"\n",
    "        Đánh giá mô hình đã lưu trên tập test.\n",
    "        Trả về: acc, loss, preds, true_labels, probs\n",
    "        \"\"\"\n",
    "        save_path = os.path.join(\"/kaggle/working\", f\"{self.model_name}_best.pt\")\n",
    "        self.model.load_state_dict(torch.load(save_path))\n",
    "        self.model.eval()\n",
    "\n",
    "        all_preds, all_labels, all_probs = [], [], []\n",
    "        total_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                outputs = self.model(**batch)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                preds = torch.argmax(probs, dim=-1)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(batch['labels'].cpu().numpy())\n",
    "                all_probs.extend(probs[:, 1].cpu().numpy() if probs.shape[1] > 1 else probs[:, 0].cpu().numpy())\n",
    "\n",
    "        avg_loss = total_loss / len(test_loader)\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "        # Save for later analysis\n",
    "        results_df = pd.DataFrame({\n",
    "            \"true_label\": all_labels,\n",
    "            \"pred_label\": all_preds,\n",
    "            \"prob_class1\": all_probs\n",
    "        })\n",
    "        results_path = f\"/kaggle/working/{self.model_name}_test_results.csv\"\n",
    "        results_df.to_csv(results_path, index=False)\n",
    "        print(f\"Test results saved to {results_path}\")\n",
    "\n",
    "        return acc, avg_loss, all_preds, all_labels, all_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T13:14:14.872056Z",
     "iopub.status.busy": "2025-06-17T13:14:14.871727Z",
     "iopub.status.idle": "2025-06-17T13:14:14.881556Z",
     "shell.execute_reply": "2025-06-17T13:14:14.880817Z",
     "shell.execute_reply.started": "2025-06-17T13:14:14.872035Z"
    },
    "id": "csCM3MzT8HMD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_model(model_name, batch_size):\n",
    "    global df, learning_rate, epochs, patience, X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "    print(\"========== LOAD TOKENIZER & MODEL ==========\")\n",
    "    tokenizer, model = get_tokenizer_and_model(model_name, num_labels=len(set(df[\"label\"])))\n",
    "\n",
    "    print(\"========== CREATE DATASETS ==========\")\n",
    "    train_dataset = TextClassificationDataset(X_train, y_train, tokenizer)\n",
    "    val_dataset = TextClassificationDataset(X_val, y_val, tokenizer)\n",
    "    test_dataset = TextClassificationDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "    print(\"========== CREATE DATALOADERS ==========\")\n",
    "    train_loader = DataLoaderBuilder(train_dataset, batch_size=batch_size, shuffle=True).get_dataloader()\n",
    "    val_loader = DataLoaderBuilder(val_dataset, batch_size=batch_size, shuffle=False).get_dataloader()\n",
    "    test_loader = DataLoaderBuilder(test_dataset, batch_size=batch_size, shuffle=False).get_dataloader()\n",
    "\n",
    "    print(\"========== INITIALIZE TRAINER ==========\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        model_name=model_name,\n",
    "        lr=learning_rate,\n",
    "        epochs=epochs,\n",
    "        patience=patience,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(f\"\\tTRAINING MODEL: {model_name}\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    train_losses, val_losses, val_accuracies = trainer.train()\n",
    "\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(f\"\\tEVALUATION ON TEST SET\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    test_acc, test_loss, test_preds, test_true_labels, test_probs = trainer.test_model(test_loader)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(test_true_labels, test_preds, target_names=[str(i) for i in sorted(df['label'].unique())]))\n",
    "\n",
    "    cm = confusion_matrix(test_true_labels, test_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=sorted(df['label'].unique()),\n",
    "                yticklabels=sorted(df['label'].unique()))\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Test Accuracy: {test_acc:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Trả về dữ liệu cho việc vẽ về sau hoặc lưu log thêm\n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"val_accuracies\": val_accuracies,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_preds\": test_preds,\n",
    "        \"test_labels\": test_true_labels,\n",
    "        \"test_probs\": test_probs\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T13:14:17.464179Z",
     "iopub.status.busy": "2025-06-17T13:14:17.463569Z"
    },
    "id": "bzQXPI5e8Kob",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== LOAD TOKENIZER & MODEL ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== CREATE DATASETS ==========\n",
      "========== CREATE DATALOADERS ==========\n",
      "========== INITIALIZE TRAINER ==========\n",
      "\n",
      "--------------------------------------------------\n",
      "\tTRAINING MODEL: bert-base-uncased\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▏         | 11/749 [00:18<19:52,  1.62s/it]"
     ]
    }
   ],
   "source": [
    "results = run_model(\"bert-base-uncased\", batch_size=64)\n",
    "\n",
    "plot_curves(results[\"train_losses\"], results[\"val_losses\"], results[\"val_accuracies\"])\n",
    "plot_roc_curve(results[\"test_labels\"], results[\"test_probs\"])\n",
    "evaluate(results[\"test_labels\"], results[\"test_preds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPFt6Fuj8MEq"
   },
   "outputs": [],
   "source": [
    "results_2 = run_model(\"roberta-base\", batch_size=64)\n",
    "\n",
    "plot_curves(results_2[\"train_losses\"], results_2[\"val_losses\"], results_2[\"val_accuracies\"])\n",
    "plot_roc_curve(results_2[\"test_labels\"], results_2[\"test_probs\"])\n",
    "evaluate(results_2[\"test_labels\"], results_2[\"test_preds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VelNmI928Opg"
   },
   "outputs": [],
   "source": [
    "results_3 = run_model(model_name=\"xlnet-base-cased\", batch_size=32)\n",
    "\n",
    "plot_curves(results_3[\"train_losses\"], results_3[\"val_losses\"], results_3[\"val_accuracies\"])\n",
    "plot_roc_curve(results_3[\"test_labels\"], results_3[\"test_probs\"])\n",
    "evaluate(results_3[\"test_labels\"], results_3[\"test_preds\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "nW7Xan1n6VtR"
   ],
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7615110,
     "sourceId": 12096450,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
